{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (GPU) kullanılacak.\n"
     ]
    }
   ],
   "source": [
    "# GPU desteği var mı kontrol et\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Metal GPU backend\n",
    "    print(\"MPS (GPU) kullanılacak.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU desteklenmiyor, CPU kullanılacak.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(path, num_img):\n",
    "    array = np.zeros((num_img, 64*32))\n",
    "    i = 0\n",
    "    for img in os.listdir(path):\n",
    "        img_path = path + '/' + img\n",
    "        img = Image.open(img_path, mode= 'r')\n",
    "        data = np.asanyarray(img, dtype= 'uint8')\n",
    "        data = data.flatten()\n",
    "        array[i,:] = data\n",
    "        i += 1\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_negative_path = r'Data/LSIFIR/Classification/Train/neg'\n",
    "num_train_negative_img = 43390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_negative_array \u001b[38;5;241m=\u001b[39m \u001b[43mread_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_negative_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_train_negative_img\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m, in \u001b[0;36mread_images\u001b[0;34m(path, num_img)\u001b[0m\n\u001b[1;32m      5\u001b[0m img_path \u001b[38;5;241m=\u001b[39m path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m img\n\u001b[1;32m      6\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path, mode\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      9\u001b[0m array[i,:] \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/Documents/python_env2/env/lib/python3.12/site-packages/PIL/Image.py:686\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"iPython display hook support for JPEG format.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    :returns: JPEG version of the image as bytes\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repr_image(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 686\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array_interface__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# numpy array interface support\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     new \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m}\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_negative_array = read_images(train_negative_path, num_train_negative_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_negative_tensor = torch.from_numpy(train_negative_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_negative_tensor = torch.zeros(num_train_negative_img, dtype= torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positive_path = r'Data/LSIFIR/Classification/Train/pos'\n",
    "num_train_positive_img = 10208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positive_array = read_images(train_positive_path, num_train_positive_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_positive_tensor = torch.from_numpy(train_positive_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_positive_tensor = torch.ones(num_train_positive_img, dtype= torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Concat Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.cat((x_train_negative_tensor,x_train_positive_tensor),0)\n",
    "y_train = torch.cat((y_train_negative_tensor,y_train_positive_tensor),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  torch.Size([53598, 2048])\n",
      "y_train:  torch.Size([53598])\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train: \",x_train.size())\n",
    "print(\"y_train: \",y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_negative_path = r'Data/LSIFIR/Classification/Test/neg'\n",
    "num_test_negative_img = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_negative_array = read_images(test_negative_path,num_test_negative_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_negative_tensor = torch.tensor(test_negative_array[:20855,:])\n",
    "y_test_negative_tensor = torch.zeros(20855, dtype= torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positive_path = r'Data/LSIFIR/Classification/Test/pos'\n",
    "num_test_positive_img = 5944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positive_array = read_images(test_positive_path,num_test_positive_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_positive_tensor = torch.tensor(test_positive_array)\n",
    "y_test_positive_tensor = torch.ones(num_test_positive_img, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Concat Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.cat((x_test_negative_tensor, x_test_positive_tensor), 0)\n",
    "y_test = torch.cat((y_test_negative_tensor, y_test_positive_tensor), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x129d95940>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[49001,:].reshape(64,32,), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "num_epochs = 5000\n",
    "num_classes = 2\n",
    "batch_size = 8933\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1,10,5)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(10,16,5)\n",
    "\n",
    "        self.fc1 = nn.Linear(16*13*5,520)\n",
    "        self.fc2 = nn.Linear(520,130)\n",
    "        self.fc3 = nn.Linear(130,num_classes)\n",
    "\n",
    "    def forward(self,x):  # x inputumuz\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x))) #conv katmanından sonra RELU, sonrada Pooling yaptık\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(-1,16*13*5)\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))  \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(x_train, y_train) # Train verilerimizi Troch için uygun formata getirme\n",
    "trainloader = torch.utils.data.DataLoader(train,batch_size= batch_size, shuffle=True ) # Uygun formata getirilmiş train setimizi hyperparametreler ile initialize etme\n",
    "\n",
    "test = torch.utils.data.TensorDataset(x_train, y_train) # Train verilerimizi Troch için uygun formata getirme\n",
    "testloader = torch.utils.data.DataLoader(test,batch_size= batch_size, shuffle=False ) # Uygun formata getirilmiş train setimizi hyperparametreler ile initialize etme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device=device)  # to fonksiyonu modeli GPUda çalıştırmak için"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr= learning_rate, momentum= 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\"> Train Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/5000\n",
      "Test accuracy:  80.94705026306951\n",
      "Train accuracy:  80.94331878055151\n",
      "epoch: 2/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 3/5000\n",
      "Test accuracy:  80.94705026306951\n",
      "Train accuracy:  80.94331878055151\n",
      "epoch: 4/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 5/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 6/5000\n",
      "Test accuracy:  80.94705026306951\n",
      "Train accuracy:  80.95078174558752\n",
      "epoch: 7/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 8/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 9/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 10/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 11/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 12/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 13/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 14/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 15/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 16/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 17/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 18/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 19/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 20/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 21/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 22/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 23/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 24/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 25/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 26/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 27/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 28/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 29/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 30/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 31/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 32/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 33/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 34/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 35/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 36/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 37/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 38/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 39/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 40/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 41/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 42/5000\n",
      "Test accuracy:  80.95451322810553\n",
      "Train accuracy:  80.95451322810553\n",
      "epoch: 43/5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \u001b[38;5;66;03m# backpropogation işlemini durdurmak için (train bitti test zamanı) ( ÖNEMLİ !!! )\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/python_env2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/python_env2/env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/python_env2/env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/python_env2/env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/python_env2/env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/python_env2/env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Documents/python_env2/env/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "loss_list = []\n",
    "\n",
    "use_gpu = True\n",
    "step_counter=0\n",
    "for epoch in range(num_epochs):\n",
    "    step_counter += 1\n",
    "    print(\"epoch: {}/{}\".format(step_counter,num_epochs))\n",
    "    for i, data in enumerate(trainloader,0):\n",
    "        inputs, labels = data # Buradaki inputun size'ı şu şekilde = [batch_size, 64*32] (bunu istemiyoruz)\n",
    "        inputs = inputs.view(batch_size, 1 , 64, 32) # reshape\n",
    "        inputs = inputs.float() \n",
    "\n",
    "        if use_gpu:\n",
    "            if torch.backends.mps.is_available():\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "        # zero gradient (Başlangıçta türevler otomatik olarak sıfırlanmadığı için elle sıfırlıyoruz)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backpropogation\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "    # test\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # backpropogation işlemini durdurmak için (train bitti test zamanı) ( ÖNEMLİ !!! )\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "\n",
    "            images = images.view(batch_size, 1, 64, 32)\n",
    "            images = images.float()\n",
    "\n",
    "            if use_gpu:\n",
    "                if torch.backends.mps.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            output = net(images)\n",
    "\n",
    "            _ , predicted = torch.max(outputs.data,1)  # Biz sadece ikinci değeri (indeksleri) kullanmak istiyoruz. İlk dönen değeri kullanmayacağımız için Python'da geleneksel olarak \"_\" değişkeni ile tutuyoruz.\n",
    "            total += labels.size(0) # sıfırıncı index\n",
    "            correct += (predicted == labels).sum().item() # item fonksiyonu çıkan sonucu gerçek intager sayıya çevirmek için\n",
    "\n",
    "    acc1 = 100*correct/total\n",
    "    print(\"Test accuracy: \", acc1)\n",
    "    test_acc.append(acc1)\n",
    "\n",
    "    # train\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad(): # backpropogation işlemini durdurmak için (train bitti test zamanı)\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "\n",
    "            images = images.view(batch_size, 1, 64, 32)\n",
    "            images = images.float()\n",
    "\n",
    "            if use_gpu:\n",
    "                if torch.backends.mps.is_available():\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            output = net(images)\n",
    "\n",
    "            _ , predicted = torch.max(outputs.data,1)  # Biz sadece ikinci değeri (indeksleri) kullanmak istiyoruz. İlk dönen değeri kullanmayacağımız için Python'da geleneksel olarak \"_\" değişkeni ile tutuyoruz.\n",
    "            total += labels.size(0) # sıfırıncı index\n",
    "            correct += (predicted == labels).sum().item() # item fonksiyonu çıkan sonucu gerçek intager sayıya çevirmek için\n",
    "\n",
    "    acc2 = 100*correct/total\n",
    "    print(\"Train accuracy: \", acc2)\n",
    "    train_acc.append(acc2)\n",
    "\n",
    "print(\"Training Done!\")\n",
    "\n",
    "end = time.time()\n",
    "process_time = (end-start)/60\n",
    "print(\"process time: \",process_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
